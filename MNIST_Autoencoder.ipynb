{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPsPGi9xFoDWywGJk5Y8N7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahsanGoheer/Advanced-Python-Workshop-AI/blob/main/MNIST_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vcZcqR7KOEH",
        "outputId": "079ef901-8d85-4120-ea5a-c31d2ccdace3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Necessary Modules.\n",
        "\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Modules\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "msiCc3mZMOJI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Mnist Dataset\n",
        " \n",
        "\n",
        "# Prepare a tensor transformation for the images.\n",
        "image_tensor_trfm = transforms.ToTensor()\n",
        "\n",
        "# Download the MNIST Dataset.\n",
        "mnist_dataset = datasets.MNIST(root = \"./data\",\n",
        "                               train = True,\n",
        "                               transform = image_tensor_trfm,\n",
        "                               download = True)\n",
        "# Create a Data Loader.\n",
        "data_loader = torch.utils.data.DataLoader(dataset = mnist_dataset,\n",
        "                                          batch_size = 32,\n",
        "                                          shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6i9CGCdNPfp",
        "outputId": "f8915dbb-bfe2-4f3a-dae3-9faf1574354d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 282225119.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 120893906.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 152921866.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 23204054.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = torch.nn.Sequential(torch.nn.Linear(28 * 28, 128),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(128, 64),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(64, 32),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(32, 16),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(16, 8)\n",
        "                                       )\n",
        "    self.decoder = torch.nn.Sequential(torch.nn.Linear(8, 16),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(16, 32),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(32, 64),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(64, 128),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(128, 28 * 28)\n",
        "                                       )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zoB9jcFzN77c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_encoder_model = AutoEncoder()\n",
        "auto_encoder_model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfSTX8VyRWB-",
        "outputId": "b7fc92f5-f817-409b-e2e4-2275bda089eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=16, out_features=8, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=64, out_features=128, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=128, out_features=784, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(auto_encoder_model.parameters(),\n",
        "                             lr = 1e-1,\n",
        "                             weight_decay = 1e-8\n",
        "                             )"
      ],
      "metadata": {
        "id": "tDeJGCRNRZDh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "6-pc08_VTV23"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "outputs = []\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch:{epoch}\")\n",
        "    counter = 1\n",
        "    for (image, _) in data_loader:\n",
        "      \n",
        "      # Reshaping the image to (-1, 784)\n",
        "      image = image.reshape(-1, 28*28).cuda()\n",
        "       \n",
        "      # Output of Autoencoder\n",
        "      reconstructed = auto_encoder_model(image)\n",
        "       \n",
        "      # Calculating the loss function\n",
        "      loss = loss_func(reconstructed, image)\n",
        "       \n",
        "      # The gradients are set to zero,\n",
        "      # the gradient is computed and stored.\n",
        "      # .step() performs parameter update\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      print(f\"Image:{counter} Loss:{loss}\")\n",
        "      counter+=1\n",
        "      # Storing the losses in a list for plotting\n",
        "      losses.append(loss)\n",
        "    outputs.append((epochs, image, reconstructed))\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "syIn5gy3R6by",
        "outputId": "931daa28-5ea8-41d7-8411-e7534c4c728d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0\n",
            "Image:1 Loss:0.06889767199754715\n",
            "Image:2 Loss:0.07444838434457779\n",
            "Image:3 Loss:0.06300678104162216\n",
            "Image:4 Loss:0.0651049017906189\n",
            "Image:5 Loss:0.061039503663778305\n",
            "Image:6 Loss:0.06955214589834213\n",
            "Image:7 Loss:0.06824430823326111\n",
            "Image:8 Loss:0.06615887582302094\n",
            "Image:9 Loss:0.07767949998378754\n",
            "Image:10 Loss:0.0712776854634285\n",
            "Image:11 Loss:0.07348691672086716\n",
            "Image:12 Loss:0.0644756630063057\n",
            "Image:13 Loss:0.07227558642625809\n",
            "Image:14 Loss:0.06776399165391922\n",
            "Image:15 Loss:0.07023893296718597\n",
            "Image:16 Loss:0.06619398295879364\n",
            "Image:17 Loss:0.06884067505598068\n",
            "Image:18 Loss:0.07003781944513321\n",
            "Image:19 Loss:0.07075932621955872\n",
            "Image:20 Loss:0.07089804112911224\n",
            "Image:21 Loss:0.06581181287765503\n",
            "Image:22 Loss:0.06670759618282318\n",
            "Image:23 Loss:0.07401058822870255\n",
            "Image:24 Loss:0.06655114889144897\n",
            "Image:25 Loss:0.06861531734466553\n",
            "Image:26 Loss:0.07156793028116226\n",
            "Image:27 Loss:0.07018686830997467\n",
            "Image:28 Loss:0.0735911950469017\n",
            "Image:29 Loss:0.07110092788934708\n",
            "Image:30 Loss:0.06849634647369385\n",
            "Image:31 Loss:0.06739658862352371\n",
            "Image:32 Loss:0.0658089742064476\n",
            "Image:33 Loss:0.06889262050390244\n",
            "Image:34 Loss:0.06769707798957825\n",
            "Image:35 Loss:0.06670767813920975\n",
            "Image:36 Loss:0.06652281433343887\n",
            "Image:37 Loss:0.06618645042181015\n",
            "Image:38 Loss:0.07019128650426865\n",
            "Image:39 Loss:0.07428369671106339\n",
            "Image:40 Loss:0.0655418112874031\n",
            "Image:41 Loss:0.06711842119693756\n",
            "Image:42 Loss:0.06471515446901321\n",
            "Image:43 Loss:0.06620696932077408\n",
            "Image:44 Loss:0.07029927521944046\n",
            "Image:45 Loss:0.07413429766893387\n",
            "Image:46 Loss:0.06605533510446548\n",
            "Image:47 Loss:0.0673462301492691\n",
            "Image:48 Loss:0.06689243018627167\n",
            "Image:49 Loss:0.0633954331278801\n",
            "Image:50 Loss:0.0682036355137825\n",
            "Image:51 Loss:0.06662037968635559\n",
            "Image:52 Loss:0.0665876492857933\n",
            "Image:53 Loss:0.07230541855096817\n",
            "Image:54 Loss:0.06292182207107544\n",
            "Image:55 Loss:0.0654522031545639\n",
            "Image:56 Loss:0.06689333915710449\n",
            "Image:57 Loss:0.07197360694408417\n",
            "Image:58 Loss:0.06723856925964355\n",
            "Image:59 Loss:0.06572222709655762\n",
            "Image:60 Loss:0.07010415196418762\n",
            "Image:61 Loss:0.06844720244407654\n",
            "Image:62 Loss:0.06815816462039948\n",
            "Image:63 Loss:0.06702113151550293\n",
            "Image:64 Loss:0.072076216340065\n",
            "Image:65 Loss:0.06517022103071213\n",
            "Image:66 Loss:0.06836892664432526\n",
            "Image:67 Loss:0.06659578531980515\n",
            "Image:68 Loss:0.06657896190881729\n",
            "Image:69 Loss:0.07607131451368332\n",
            "Image:70 Loss:0.0649280846118927\n",
            "Image:71 Loss:0.06361684948205948\n",
            "Image:72 Loss:0.06741015613079071\n",
            "Image:73 Loss:0.06935656815767288\n",
            "Image:74 Loss:0.06865783035755157\n",
            "Image:75 Loss:0.06837423145771027\n",
            "Image:76 Loss:0.06823946535587311\n",
            "Image:77 Loss:0.07205475121736526\n",
            "Image:78 Loss:0.07083107531070709\n",
            "Image:79 Loss:0.06514806300401688\n",
            "Image:80 Loss:0.07080285996198654\n",
            "Image:81 Loss:0.07688954472541809\n",
            "Image:82 Loss:0.06535029411315918\n",
            "Image:83 Loss:0.06533639878034592\n",
            "Image:84 Loss:0.0679163932800293\n",
            "Image:85 Loss:0.06405764818191528\n",
            "Image:86 Loss:0.07066403329372406\n",
            "Image:87 Loss:0.06715371459722519\n",
            "Image:88 Loss:0.06518448144197464\n",
            "Image:89 Loss:0.061973657459020615\n",
            "Image:90 Loss:0.06737776100635529\n",
            "Image:91 Loss:0.07094276696443558\n",
            "Image:92 Loss:0.07167837768793106\n",
            "Image:93 Loss:0.06660190969705582\n",
            "Image:94 Loss:0.06525853276252747\n",
            "Image:95 Loss:0.06762628257274628\n",
            "Image:96 Loss:0.0683935210108757\n",
            "Image:97 Loss:0.059165164828300476\n",
            "Image:98 Loss:0.07123290747404099\n",
            "Image:99 Loss:0.07449613511562347\n",
            "Image:100 Loss:0.0700669139623642\n",
            "Image:101 Loss:0.06479054689407349\n",
            "Image:102 Loss:0.062191884964704514\n",
            "Image:103 Loss:0.06586239486932755\n",
            "Image:104 Loss:0.06415624171495438\n",
            "Image:105 Loss:0.07519961893558502\n",
            "Image:106 Loss:0.07443667948246002\n",
            "Image:107 Loss:0.07637716829776764\n",
            "Image:108 Loss:0.06501329690217972\n",
            "Image:109 Loss:0.07604476064443588\n",
            "Image:110 Loss:0.06376202404499054\n",
            "Image:111 Loss:0.0660877451300621\n",
            "Image:112 Loss:0.06727885454893112\n",
            "Image:113 Loss:0.0689271092414856\n",
            "Image:114 Loss:0.07076165825128555\n",
            "Image:115 Loss:0.06731303781270981\n",
            "Image:116 Loss:0.06703149527311325\n",
            "Image:117 Loss:0.07568537443876266\n",
            "Image:118 Loss:0.06350165605545044\n",
            "Image:119 Loss:0.06731107085943222\n",
            "Image:120 Loss:0.0648135170340538\n",
            "Image:121 Loss:0.06836547702550888\n",
            "Image:122 Loss:0.06412725895643234\n",
            "Image:123 Loss:0.07509271055459976\n",
            "Image:124 Loss:0.05809779092669487\n",
            "Image:125 Loss:0.0705321654677391\n",
            "Image:126 Loss:0.07099924236536026\n",
            "Image:127 Loss:0.06961290538311005\n",
            "Image:128 Loss:0.06687517464160919\n",
            "Image:129 Loss:0.0682976022362709\n",
            "Image:130 Loss:0.07621029764413834\n",
            "Image:131 Loss:0.0676887184381485\n",
            "Image:132 Loss:0.07013153284788132\n",
            "Image:133 Loss:0.06919190287590027\n",
            "Image:134 Loss:0.07241133600473404\n",
            "Image:135 Loss:0.06775780767202377\n",
            "Image:136 Loss:0.07029359042644501\n",
            "Image:137 Loss:0.06492075324058533\n",
            "Image:138 Loss:0.06408054381608963\n",
            "Image:139 Loss:0.06885351985692978\n",
            "Image:140 Loss:0.07106006890535355\n",
            "Image:141 Loss:0.07209473848342896\n",
            "Image:142 Loss:0.07374745607376099\n",
            "Image:143 Loss:0.06816056370735168\n",
            "Image:144 Loss:0.06785470992326736\n",
            "Image:145 Loss:0.06327810138463974\n",
            "Image:146 Loss:0.06543421745300293\n",
            "Image:147 Loss:0.07021932303905487\n",
            "Image:148 Loss:0.06591248512268066\n",
            "Image:149 Loss:0.07402563840150833\n",
            "Image:150 Loss:0.06664231419563293\n",
            "Image:151 Loss:0.06725864112377167\n",
            "Image:152 Loss:0.06586203724145889\n",
            "Image:153 Loss:0.07300005853176117\n",
            "Image:154 Loss:0.07229439914226532\n",
            "Image:155 Loss:0.06759463995695114\n",
            "Image:156 Loss:0.0663352757692337\n",
            "Image:157 Loss:0.07059790194034576\n",
            "Image:158 Loss:0.07523611187934875\n",
            "Image:159 Loss:0.06816477328538895\n",
            "Image:160 Loss:0.06422467529773712\n",
            "Image:161 Loss:0.06740780174732208\n",
            "Image:162 Loss:0.06684097647666931\n",
            "Image:163 Loss:0.06752662360668182\n",
            "Image:164 Loss:0.07104307413101196\n",
            "Image:165 Loss:0.0710204467177391\n",
            "Image:166 Loss:0.06940599530935287\n",
            "Image:167 Loss:0.06881319731473923\n",
            "Image:168 Loss:0.06433038413524628\n",
            "Image:169 Loss:0.06867042928934097\n",
            "Image:170 Loss:0.062487754970788956\n",
            "Image:171 Loss:0.0703451856970787\n",
            "Image:172 Loss:0.06979674845933914\n",
            "Image:173 Loss:0.06619486212730408\n",
            "Image:174 Loss:0.06878980994224548\n",
            "Image:175 Loss:0.07201538234949112\n",
            "Image:176 Loss:0.0726172924041748\n",
            "Image:177 Loss:0.06874478608369827\n",
            "Image:178 Loss:0.06862838566303253\n",
            "Image:179 Loss:0.07046567648649216\n",
            "Image:180 Loss:0.07284272462129593\n",
            "Image:181 Loss:0.06999959796667099\n",
            "Image:182 Loss:0.06514132767915726\n",
            "Image:183 Loss:0.06883566826581955\n",
            "Image:184 Loss:0.06647945940494537\n",
            "Image:185 Loss:0.06730462610721588\n",
            "Image:186 Loss:0.06754042208194733\n",
            "Image:187 Loss:0.06816097348928452\n",
            "Image:188 Loss:0.06166303902864456\n",
            "Image:189 Loss:0.06907551735639572\n",
            "Image:190 Loss:0.07290995866060257\n",
            "Image:191 Loss:0.06416654586791992\n",
            "Image:192 Loss:0.06597907096147537\n",
            "Image:193 Loss:0.06682074815034866\n",
            "Image:194 Loss:0.06956526637077332\n",
            "Image:195 Loss:0.06583337485790253\n",
            "Image:196 Loss:0.06685119867324829\n",
            "Image:197 Loss:0.07150379568338394\n",
            "Image:198 Loss:0.07180028408765793\n",
            "Image:199 Loss:0.06982054561376572\n",
            "Image:200 Loss:0.06303520500659943\n",
            "Image:201 Loss:0.06910937279462814\n",
            "Image:202 Loss:0.07422605901956558\n",
            "Image:203 Loss:0.06675689667463303\n",
            "Image:204 Loss:0.06732126325368881\n",
            "Image:205 Loss:0.06628742069005966\n",
            "Image:206 Loss:0.0670771673321724\n",
            "Image:207 Loss:0.06257176399230957\n",
            "Image:208 Loss:0.06371346116065979\n",
            "Image:209 Loss:0.06802617013454437\n",
            "Image:210 Loss:0.06727148592472076\n",
            "Image:211 Loss:0.06871969997882843\n",
            "Image:212 Loss:0.07310553640127182\n",
            "Image:213 Loss:0.0668712630867958\n",
            "Image:214 Loss:0.06077900156378746\n",
            "Image:215 Loss:0.07278710603713989\n",
            "Image:216 Loss:0.06508033722639084\n",
            "Image:217 Loss:0.06924882531166077\n",
            "Image:218 Loss:0.06752731651067734\n",
            "Image:219 Loss:0.06958993524312973\n",
            "Image:220 Loss:0.070708729326725\n",
            "Image:221 Loss:0.07249785959720612\n",
            "Image:222 Loss:0.06579529494047165\n",
            "Image:223 Loss:0.06887151300907135\n",
            "Image:224 Loss:0.06615587323904037\n",
            "Image:225 Loss:0.07316558808088303\n",
            "Image:226 Loss:0.06342882663011551\n",
            "Image:227 Loss:0.06992222368717194\n",
            "Image:228 Loss:0.07062986493110657\n",
            "Image:229 Loss:0.07221546024084091\n",
            "Image:230 Loss:0.06510132551193237\n",
            "Image:231 Loss:0.0740237608551979\n",
            "Image:232 Loss:0.06716497242450714\n",
            "Image:233 Loss:0.07052015513181686\n",
            "Image:234 Loss:0.06925073266029358\n",
            "Image:235 Loss:0.07059124112129211\n",
            "Image:236 Loss:0.07135830074548721\n",
            "Image:237 Loss:0.06745163351297379\n",
            "Image:238 Loss:0.07131269574165344\n",
            "Image:239 Loss:0.0716938003897667\n",
            "Image:240 Loss:0.06787875294685364\n",
            "Image:241 Loss:0.0659906193614006\n",
            "Image:242 Loss:0.07334700971841812\n",
            "Image:243 Loss:0.07036644220352173\n",
            "Image:244 Loss:0.07233035564422607\n",
            "Image:245 Loss:0.06863817572593689\n",
            "Image:246 Loss:0.06847149133682251\n",
            "Image:247 Loss:0.06894286721944809\n",
            "Image:248 Loss:0.06390615552663803\n",
            "Image:249 Loss:0.0629977136850357\n",
            "Image:250 Loss:0.07067261636257172\n",
            "Image:251 Loss:0.07034188508987427\n",
            "Image:252 Loss:0.06282717734575272\n",
            "Image:253 Loss:0.071368508040905\n",
            "Image:254 Loss:0.07306534796953201\n",
            "Image:255 Loss:0.06906227767467499\n",
            "Image:256 Loss:0.06673017889261246\n",
            "Image:257 Loss:0.06376565992832184\n",
            "Image:258 Loss:0.07029202580451965\n",
            "Image:259 Loss:0.06839743256568909\n",
            "Image:260 Loss:0.06706304848194122\n",
            "Image:261 Loss:0.0634940043091774\n",
            "Image:262 Loss:0.06413249671459198\n",
            "Image:263 Loss:0.06560783088207245\n",
            "Image:264 Loss:0.066609226167202\n",
            "Image:265 Loss:0.06468231976032257\n",
            "Image:266 Loss:0.0649159699678421\n",
            "Image:267 Loss:0.06880294531583786\n",
            "Image:268 Loss:0.0710921436548233\n",
            "Image:269 Loss:0.06468960642814636\n",
            "Image:270 Loss:0.06780385971069336\n",
            "Image:271 Loss:0.07190492749214172\n",
            "Image:272 Loss:0.07588040083646774\n",
            "Image:273 Loss:0.06603045761585236\n",
            "Image:274 Loss:0.06837871670722961\n",
            "Image:275 Loss:0.06489016115665436\n",
            "Image:276 Loss:0.0713324174284935\n",
            "Image:277 Loss:0.06724327057600021\n",
            "Image:278 Loss:0.06585360318422318\n",
            "Image:279 Loss:0.0718047246336937\n",
            "Image:280 Loss:0.07132305204868317\n",
            "Image:281 Loss:0.06769879162311554\n",
            "Image:282 Loss:0.06984242051839828\n",
            "Image:283 Loss:0.06862125545740128\n",
            "Image:284 Loss:0.06912177056074142\n",
            "Image:285 Loss:0.07744406163692474\n",
            "Image:286 Loss:0.0673358291387558\n",
            "Image:287 Loss:0.06443854421377182\n",
            "Image:288 Loss:0.06664387136697769\n",
            "Image:289 Loss:0.06810116767883301\n",
            "Image:290 Loss:0.06795591861009598\n",
            "Image:291 Loss:0.06295999884605408\n",
            "Image:292 Loss:0.0726640447974205\n",
            "Image:293 Loss:0.06782690435647964\n",
            "Image:294 Loss:0.07598782330751419\n",
            "Image:295 Loss:0.06798084080219269\n",
            "Image:296 Loss:0.07496125251054764\n",
            "Image:297 Loss:0.06778553873300552\n",
            "Image:298 Loss:0.06876864284276962\n",
            "Image:299 Loss:0.06770884245634079\n",
            "Image:300 Loss:0.07383333891630173\n",
            "Image:301 Loss:0.07072493433952332\n",
            "Image:302 Loss:0.06958857923746109\n",
            "Image:303 Loss:0.06680411845445633\n",
            "Image:304 Loss:0.0701104924082756\n",
            "Image:305 Loss:0.0663309097290039\n",
            "Image:306 Loss:0.07058309763669968\n",
            "Image:307 Loss:0.07122992724180222\n",
            "Image:308 Loss:0.06726010143756866\n",
            "Image:309 Loss:0.0645703449845314\n",
            "Image:310 Loss:0.06974229961633682\n",
            "Image:311 Loss:0.07155641913414001\n",
            "Image:312 Loss:0.06535714119672775\n",
            "Image:313 Loss:0.07546912133693695\n",
            "Image:314 Loss:0.06455567479133606\n",
            "Image:315 Loss:0.07400772720575333\n",
            "Image:316 Loss:0.07182445377111435\n",
            "Image:317 Loss:0.06897471845149994\n",
            "Image:318 Loss:0.06853240728378296\n",
            "Image:319 Loss:0.06773514300584793\n",
            "Image:320 Loss:0.07028155028820038\n",
            "Image:321 Loss:0.06478633731603622\n",
            "Image:322 Loss:0.06551025062799454\n",
            "Image:323 Loss:0.06507717072963715\n",
            "Image:324 Loss:0.07528712600469589\n",
            "Image:325 Loss:0.07050995528697968\n",
            "Image:326 Loss:0.06782425940036774\n",
            "Image:327 Loss:0.06677686423063278\n",
            "Image:328 Loss:0.0701225996017456\n",
            "Image:329 Loss:0.06868559867143631\n",
            "Image:330 Loss:0.07094014436006546\n",
            "Image:331 Loss:0.06629661470651627\n",
            "Image:332 Loss:0.06607789546251297\n",
            "Image:333 Loss:0.07034563273191452\n",
            "Image:334 Loss:0.0671817883849144\n",
            "Image:335 Loss:0.06681293994188309\n",
            "Image:336 Loss:0.06550172716379166\n",
            "Image:337 Loss:0.06450314074754715\n",
            "Image:338 Loss:0.07207418978214264\n",
            "Image:339 Loss:0.06611877679824829\n",
            "Image:340 Loss:0.0675518661737442\n",
            "Image:341 Loss:0.0638389140367508\n",
            "Image:342 Loss:0.06559155136346817\n",
            "Image:343 Loss:0.07750030606985092\n",
            "Image:344 Loss:0.0770619809627533\n",
            "Image:345 Loss:0.06617489457130432\n",
            "Image:346 Loss:0.0721321776509285\n",
            "Image:347 Loss:0.062481265515089035\n",
            "Image:348 Loss:0.06634878367185593\n",
            "Image:349 Loss:0.0712345540523529\n",
            "Image:350 Loss:0.06439737230539322\n",
            "Image:351 Loss:0.06532152742147446\n",
            "Image:352 Loss:0.07296210527420044\n",
            "Image:353 Loss:0.06839988380670547\n",
            "Image:354 Loss:0.07524672150611877\n",
            "Image:355 Loss:0.06559614092111588\n",
            "Image:356 Loss:0.07018814980983734\n",
            "Image:357 Loss:0.07135508209466934\n",
            "Image:358 Loss:0.06791885197162628\n",
            "Image:359 Loss:0.06634995341300964\n",
            "Image:360 Loss:0.06695402413606644\n",
            "Image:361 Loss:0.0664195641875267\n",
            "Image:362 Loss:0.06765206158161163\n",
            "Image:363 Loss:0.06737913936376572\n",
            "Image:364 Loss:0.06632629781961441\n",
            "Image:365 Loss:0.0743645578622818\n",
            "Image:366 Loss:0.0692218467593193\n",
            "Image:367 Loss:0.06588475406169891\n",
            "Image:368 Loss:0.06923849880695343\n",
            "Image:369 Loss:0.06743182986974716\n",
            "Image:370 Loss:0.06564821302890778\n",
            "Image:371 Loss:0.07386051118373871\n",
            "Image:372 Loss:0.07014680653810501\n",
            "Image:373 Loss:0.07181574404239655\n",
            "Image:374 Loss:0.07083098590373993\n",
            "Image:375 Loss:0.06572439521551132\n",
            "Image:376 Loss:0.0694064125418663\n",
            "Image:377 Loss:0.07437176257371902\n",
            "Image:378 Loss:0.0719112828373909\n",
            "Image:379 Loss:0.07571011781692505\n",
            "Image:380 Loss:0.06226798892021179\n",
            "Image:381 Loss:0.07081107795238495\n",
            "Image:382 Loss:0.06323684751987457\n",
            "Image:383 Loss:0.07068789750337601\n",
            "Image:384 Loss:0.0816679447889328\n",
            "Image:385 Loss:0.07330283522605896\n",
            "Image:386 Loss:0.06913772970438004\n",
            "Image:387 Loss:0.06699305772781372\n",
            "Image:388 Loss:0.06752435117959976\n",
            "Image:389 Loss:0.07309292256832123\n",
            "Image:390 Loss:0.06907770782709122\n",
            "Image:391 Loss:0.06688147038221359\n",
            "Image:392 Loss:0.06539463996887207\n",
            "Image:393 Loss:0.06803108751773834\n",
            "Image:394 Loss:0.0644407868385315\n",
            "Image:395 Loss:0.07661005109548569\n",
            "Image:396 Loss:0.06799767911434174\n",
            "Image:397 Loss:0.06918927282094955\n",
            "Image:398 Loss:0.07417845726013184\n",
            "Image:399 Loss:0.06759864836931229\n",
            "Image:400 Loss:0.06449617445468903\n",
            "Image:401 Loss:0.07261239737272263\n",
            "Image:402 Loss:0.06945715844631195\n",
            "Image:403 Loss:0.0682806745171547\n",
            "Image:404 Loss:0.06376803666353226\n",
            "Image:405 Loss:0.06824024766683578\n",
            "Image:406 Loss:0.06697959452867508\n",
            "Image:407 Loss:0.0635976642370224\n",
            "Image:408 Loss:0.07040643692016602\n",
            "Image:409 Loss:0.06496257334947586\n",
            "Image:410 Loss:0.06714675575494766\n",
            "Image:411 Loss:0.06365109235048294\n",
            "Image:412 Loss:0.06694546341896057\n",
            "Image:413 Loss:0.06676941365003586\n",
            "Image:414 Loss:0.06441974639892578\n",
            "Image:415 Loss:0.0705854669213295\n",
            "Image:416 Loss:0.07190459221601486\n",
            "Image:417 Loss:0.07282394170761108\n",
            "Image:418 Loss:0.07047028839588165\n",
            "Image:419 Loss:0.06703809648752213\n",
            "Image:420 Loss:0.06819506734609604\n",
            "Image:421 Loss:0.07121097296476364\n",
            "Image:422 Loss:0.07361733913421631\n",
            "Image:423 Loss:0.07124248147010803\n",
            "Image:424 Loss:0.0657934844493866\n",
            "Image:425 Loss:0.06920419633388519\n",
            "Image:426 Loss:0.06728776544332504\n",
            "Image:427 Loss:0.06236385554075241\n",
            "Image:428 Loss:0.06767237186431885\n",
            "Image:429 Loss:0.0687817707657814\n",
            "Image:430 Loss:0.0728854313492775\n",
            "Image:431 Loss:0.07195158302783966\n",
            "Image:432 Loss:0.07154345512390137\n",
            "Image:433 Loss:0.0620708242058754\n",
            "Image:434 Loss:0.06827521324157715\n",
            "Image:435 Loss:0.07316520065069199\n",
            "Image:436 Loss:0.06372108310461044\n",
            "Image:437 Loss:0.0652419850230217\n",
            "Image:438 Loss:0.06740880012512207\n",
            "Image:439 Loss:0.06528334319591522\n",
            "Image:440 Loss:0.06636156886816025\n",
            "Image:441 Loss:0.06412525475025177\n",
            "Image:442 Loss:0.06385745853185654\n",
            "Image:443 Loss:0.07164505869150162\n",
            "Image:444 Loss:0.07362496852874756\n",
            "Image:445 Loss:0.07051083445549011\n",
            "Image:446 Loss:0.06875108927488327\n",
            "Image:447 Loss:0.06544362008571625\n",
            "Image:448 Loss:0.06309986114501953\n",
            "Image:449 Loss:0.06974322348833084\n",
            "Image:450 Loss:0.0719093382358551\n",
            "Image:451 Loss:0.06274811923503876\n",
            "Image:452 Loss:0.06240915507078171\n",
            "Image:453 Loss:0.0705622062087059\n",
            "Image:454 Loss:0.06228296086192131\n",
            "Image:455 Loss:0.07009334862232208\n",
            "Image:456 Loss:0.06993867456912994\n",
            "Image:457 Loss:0.06327543407678604\n",
            "Image:458 Loss:0.07196974754333496\n",
            "Image:459 Loss:0.06811780482530594\n",
            "Image:460 Loss:0.0711803212761879\n",
            "Image:461 Loss:0.06959183514118195\n",
            "Image:462 Loss:0.07011624425649643\n",
            "Image:463 Loss:0.07186969369649887\n",
            "Image:464 Loss:0.06554292887449265\n",
            "Image:465 Loss:0.06959706544876099\n",
            "Image:466 Loss:0.07274328917264938\n",
            "Image:467 Loss:0.06817451864480972\n",
            "Image:468 Loss:0.06688318401575089\n",
            "Image:469 Loss:0.061901338398456573\n",
            "Image:470 Loss:0.06968793272972107\n",
            "Image:471 Loss:0.06715874373912811\n",
            "Image:472 Loss:0.0628390908241272\n",
            "Image:473 Loss:0.07012973725795746\n",
            "Image:474 Loss:0.06763796508312225\n",
            "Image:475 Loss:0.0623459592461586\n",
            "Image:476 Loss:0.07050706446170807\n",
            "Image:477 Loss:0.06828654557466507\n",
            "Image:478 Loss:0.06794574856758118\n",
            "Image:479 Loss:0.06512810289859772\n",
            "Image:480 Loss:0.07051865756511688\n",
            "Image:481 Loss:0.0676582083106041\n",
            "Image:482 Loss:0.07617146521806717\n",
            "Image:483 Loss:0.0656801089644432\n",
            "Image:484 Loss:0.06544002890586853\n",
            "Image:485 Loss:0.06833158433437347\n",
            "Image:486 Loss:0.06602150201797485\n",
            "Image:487 Loss:0.06250377744436264\n",
            "Image:488 Loss:0.07322917133569717\n",
            "Image:489 Loss:0.07308995723724365\n",
            "Image:490 Loss:0.07275880128145218\n",
            "Image:491 Loss:0.06993688642978668\n",
            "Image:492 Loss:0.07108698785305023\n",
            "Image:493 Loss:0.07011410593986511\n",
            "Image:494 Loss:0.07044429332017899\n",
            "Image:495 Loss:0.06709928810596466\n",
            "Image:496 Loss:0.06879965960979462\n",
            "Image:497 Loss:0.07264941185712814\n",
            "Image:498 Loss:0.07185666263103485\n",
            "Image:499 Loss:0.06548020988702774\n",
            "Image:500 Loss:0.0688629075884819\n",
            "Image:501 Loss:0.06543594598770142\n",
            "Image:502 Loss:0.06333736330270767\n",
            "Image:503 Loss:0.07414769381284714\n",
            "Image:504 Loss:0.0686958059668541\n",
            "Image:505 Loss:0.06707405298948288\n",
            "Image:506 Loss:0.0694800540804863\n",
            "Image:507 Loss:0.07078566402196884\n",
            "Image:508 Loss:0.07061472535133362\n",
            "Image:509 Loss:0.06377266347408295\n",
            "Image:510 Loss:0.07081009447574615\n",
            "Image:511 Loss:0.06890372931957245\n",
            "Image:512 Loss:0.07147786021232605\n",
            "Image:513 Loss:0.07051147520542145\n",
            "Image:514 Loss:0.06831686943769455\n",
            "Image:515 Loss:0.07027745246887207\n",
            "Image:516 Loss:0.0684853047132492\n",
            "Image:517 Loss:0.07203098386526108\n",
            "Image:518 Loss:0.06859266012907028\n",
            "Image:519 Loss:0.07767830789089203\n",
            "Image:520 Loss:0.06720513105392456\n",
            "Image:521 Loss:0.06981988996267319\n",
            "Image:522 Loss:0.07327616959810257\n",
            "Image:523 Loss:0.06710182875394821\n",
            "Image:524 Loss:0.06495679169893265\n",
            "Image:525 Loss:0.07172271609306335\n",
            "Image:526 Loss:0.06640731543302536\n",
            "Image:527 Loss:0.06552692502737045\n",
            "Image:528 Loss:0.07266691327095032\n",
            "Image:529 Loss:0.06678371131420135\n",
            "Image:530 Loss:0.06840462237596512\n",
            "Image:531 Loss:0.06908944994211197\n",
            "Image:532 Loss:0.0632486641407013\n",
            "Image:533 Loss:0.06581040471792221\n",
            "Image:534 Loss:0.07215866446495056\n",
            "Image:535 Loss:0.06773992627859116\n",
            "Image:536 Loss:0.06632289290428162\n",
            "Image:537 Loss:0.060750141739845276\n",
            "Image:538 Loss:0.06940721720457077\n",
            "Image:539 Loss:0.06878858804702759\n",
            "Image:540 Loss:0.06626393646001816\n",
            "Image:541 Loss:0.06264100968837738\n",
            "Image:542 Loss:0.07042450457811356\n",
            "Image:543 Loss:0.06358417868614197\n",
            "Image:544 Loss:0.06301894038915634\n",
            "Image:545 Loss:0.06612922996282578\n",
            "Image:546 Loss:0.063500314950943\n",
            "Image:547 Loss:0.06997621059417725\n",
            "Image:548 Loss:0.06538181751966476\n",
            "Image:549 Loss:0.07115490734577179\n",
            "Image:550 Loss:0.07541632652282715\n",
            "Image:551 Loss:0.0722232237458229\n",
            "Image:552 Loss:0.07291092723608017\n",
            "Image:553 Loss:0.07268920540809631\n",
            "Image:554 Loss:0.06786026060581207\n",
            "Image:555 Loss:0.0679669976234436\n",
            "Image:556 Loss:0.07055483758449554\n",
            "Image:557 Loss:0.0720670223236084\n",
            "Image:558 Loss:0.06599429249763489\n",
            "Image:559 Loss:0.06684450805187225\n",
            "Image:560 Loss:0.06685266643762589\n",
            "Image:561 Loss:0.06514640152454376\n",
            "Image:562 Loss:0.06545225530862808\n",
            "Image:563 Loss:0.06490723788738251\n",
            "Image:564 Loss:0.07567321509122849\n",
            "Image:565 Loss:0.061595283448696136\n",
            "Image:566 Loss:0.07394348084926605\n",
            "Image:567 Loss:0.07818140089511871\n",
            "Image:568 Loss:0.07430402934551239\n",
            "Image:569 Loss:0.07065743952989578\n",
            "Image:570 Loss:0.06791018694639206\n",
            "Image:571 Loss:0.06888478249311447\n",
            "Image:572 Loss:0.07092607766389847\n",
            "Image:573 Loss:0.06884319335222244\n",
            "Image:574 Loss:0.07195596396923065\n",
            "Image:575 Loss:0.07157114148139954\n",
            "Image:576 Loss:0.07016049325466156\n",
            "Image:577 Loss:0.06365026533603668\n",
            "Image:578 Loss:0.06965663284063339\n",
            "Image:579 Loss:0.07360886037349701\n",
            "Image:580 Loss:0.06681139767169952\n",
            "Image:581 Loss:0.06695561856031418\n",
            "Image:582 Loss:0.0669887587428093\n",
            "Image:583 Loss:0.07507339864969254\n",
            "Image:584 Loss:0.07128264009952545\n",
            "Image:585 Loss:0.07196983695030212\n",
            "Image:586 Loss:0.07189352810382843\n",
            "Image:587 Loss:0.06734040379524231\n",
            "Image:588 Loss:0.06722404807806015\n",
            "Image:589 Loss:0.06443455815315247\n",
            "Image:590 Loss:0.06565796583890915\n",
            "Image:591 Loss:0.06824753433465958\n",
            "Image:592 Loss:0.06783009320497513\n",
            "Image:593 Loss:0.07121129333972931\n",
            "Image:594 Loss:0.0710941031575203\n",
            "Image:595 Loss:0.06754732131958008\n",
            "Image:596 Loss:0.06660179793834686\n",
            "Image:597 Loss:0.07070478796958923\n",
            "Image:598 Loss:0.06711985915899277\n",
            "Image:599 Loss:0.07070541381835938\n",
            "Image:600 Loss:0.07798712700605392\n",
            "Image:601 Loss:0.06934033334255219\n",
            "Image:602 Loss:0.06646452844142914\n",
            "Image:603 Loss:0.06633292138576508\n",
            "Image:604 Loss:0.07167728245258331\n",
            "Image:605 Loss:0.06994608789682388\n",
            "Image:606 Loss:0.0704326257109642\n",
            "Image:607 Loss:0.0679406002163887\n",
            "Image:608 Loss:0.07270684093236923\n",
            "Image:609 Loss:0.06746131181716919\n",
            "Image:610 Loss:0.07323459535837173\n",
            "Image:611 Loss:0.06613031774759293\n",
            "Image:612 Loss:0.0709591880440712\n",
            "Image:613 Loss:0.07056260108947754\n",
            "Image:614 Loss:0.06922531127929688\n",
            "Image:615 Loss:0.06381898373365402\n",
            "Image:616 Loss:0.06978669762611389\n",
            "Image:617 Loss:0.06979243457317352\n",
            "Image:618 Loss:0.06585537642240524\n",
            "Image:619 Loss:0.06925736367702484\n",
            "Image:620 Loss:0.07063175737857819\n",
            "Image:621 Loss:0.06642699241638184\n",
            "Image:622 Loss:0.06985200941562653\n",
            "Image:623 Loss:0.06510988622903824\n",
            "Image:624 Loss:0.06971718370914459\n",
            "Image:625 Loss:0.07722999155521393\n",
            "Image:626 Loss:0.0630461797118187\n",
            "Image:627 Loss:0.07271944731473923\n",
            "Image:628 Loss:0.07223285734653473\n",
            "Image:629 Loss:0.06863611191511154\n",
            "Image:630 Loss:0.06849328428506851\n",
            "Image:631 Loss:0.06863461434841156\n",
            "Image:632 Loss:0.07087010890245438\n",
            "Image:633 Loss:0.06592709571123123\n",
            "Image:634 Loss:0.06411075592041016\n",
            "Image:635 Loss:0.0712762251496315\n",
            "Image:636 Loss:0.0701994076371193\n",
            "Image:637 Loss:0.06712052971124649\n",
            "Image:638 Loss:0.06458643078804016\n",
            "Image:639 Loss:0.06940320134162903\n",
            "Image:640 Loss:0.07219856977462769\n",
            "Image:641 Loss:0.06792820245027542\n",
            "Image:642 Loss:0.06837690621614456\n",
            "Image:643 Loss:0.07166694104671478\n",
            "Image:644 Loss:0.06951314210891724\n",
            "Image:645 Loss:0.068613700568676\n",
            "Image:646 Loss:0.06806853413581848\n",
            "Image:647 Loss:0.06463692337274551\n",
            "Image:648 Loss:0.07039932161569595\n",
            "Image:649 Loss:0.07017243653535843\n",
            "Image:650 Loss:0.0681162029504776\n",
            "Image:651 Loss:0.06954687088727951\n",
            "Image:652 Loss:0.0676148533821106\n",
            "Image:653 Loss:0.06713491678237915\n",
            "Image:654 Loss:0.06508225202560425\n",
            "Image:655 Loss:0.07621125876903534\n",
            "Image:656 Loss:0.0650976151227951\n",
            "Image:657 Loss:0.07406237721443176\n",
            "Image:658 Loss:0.07293268293142319\n",
            "Image:659 Loss:0.06141909956932068\n",
            "Image:660 Loss:0.06808386743068695\n",
            "Image:661 Loss:0.06954074651002884\n",
            "Image:662 Loss:0.07358458638191223\n",
            "Image:663 Loss:0.07084457576274872\n",
            "Image:664 Loss:0.07313677668571472\n",
            "Image:665 Loss:0.0640767365694046\n",
            "Image:666 Loss:0.06840630620718002\n",
            "Image:667 Loss:0.07083060592412949\n",
            "Image:668 Loss:0.07063931971788406\n",
            "Image:669 Loss:0.06819440424442291\n",
            "Image:670 Loss:0.06539136171340942\n",
            "Image:671 Loss:0.06534474343061447\n",
            "Image:672 Loss:0.07131077349185944\n",
            "Image:673 Loss:0.06901010870933533\n",
            "Image:674 Loss:0.06776224076747894\n",
            "Image:675 Loss:0.06772995740175247\n",
            "Image:676 Loss:0.06699422001838684\n",
            "Image:677 Loss:0.07218268513679504\n",
            "Image:678 Loss:0.06871449202299118\n",
            "Image:679 Loss:0.0648496001958847\n",
            "Image:680 Loss:0.07165259122848511\n",
            "Image:681 Loss:0.0642639771103859\n",
            "Image:682 Loss:0.07376613467931747\n",
            "Image:683 Loss:0.07216814905405045\n",
            "Image:684 Loss:0.06962961703538895\n",
            "Image:685 Loss:0.07407877594232559\n",
            "Image:686 Loss:0.06841248273849487\n",
            "Image:687 Loss:0.06465625762939453\n",
            "Image:688 Loss:0.06753851473331451\n",
            "Image:689 Loss:0.07343865931034088\n",
            "Image:690 Loss:0.06980317085981369\n",
            "Image:691 Loss:0.07023733109235764\n",
            "Image:692 Loss:0.06766276061534882\n",
            "Image:693 Loss:0.0703590139746666\n",
            "Image:694 Loss:0.07250528782606125\n",
            "Image:695 Loss:0.07163282483816147\n",
            "Image:696 Loss:0.06816704571247101\n",
            "Image:697 Loss:0.06423068791627884\n",
            "Image:698 Loss:0.06937829405069351\n",
            "Image:699 Loss:0.06595902144908905\n",
            "Image:700 Loss:0.0691922977566719\n",
            "Image:701 Loss:0.069516621530056\n",
            "Image:702 Loss:0.0666508600115776\n",
            "Image:703 Loss:0.0658041387796402\n",
            "Image:704 Loss:0.06430148333311081\n",
            "Image:705 Loss:0.06827768683433533\n",
            "Image:706 Loss:0.07714599370956421\n",
            "Image:707 Loss:0.06302057951688766\n",
            "Image:708 Loss:0.06397955119609833\n",
            "Image:709 Loss:0.0699327141046524\n",
            "Image:710 Loss:0.06005014479160309\n",
            "Image:711 Loss:0.07507380843162537\n",
            "Image:712 Loss:0.07243936508893967\n",
            "Image:713 Loss:0.0730091854929924\n",
            "Image:714 Loss:0.07163761556148529\n",
            "Image:715 Loss:0.07098634541034698\n",
            "Image:716 Loss:0.072749063372612\n",
            "Image:717 Loss:0.0714680477976799\n",
            "Image:718 Loss:0.06693465262651443\n",
            "Image:719 Loss:0.06557338684797287\n",
            "Image:720 Loss:0.07271326333284378\n",
            "Image:721 Loss:0.07429493218660355\n",
            "Image:722 Loss:0.06714136898517609\n",
            "Image:723 Loss:0.0648084431886673\n",
            "Image:724 Loss:0.06849965453147888\n",
            "Image:725 Loss:0.0651848167181015\n",
            "Image:726 Loss:0.06855364143848419\n",
            "Image:727 Loss:0.071809783577919\n",
            "Image:728 Loss:0.06463164836168289\n",
            "Image:729 Loss:0.07165302336215973\n",
            "Image:730 Loss:0.06686665862798691\n",
            "Image:731 Loss:0.06989549100399017\n",
            "Image:732 Loss:0.06262592226266861\n",
            "Image:733 Loss:0.06547978520393372\n",
            "Image:734 Loss:0.07380364090204239\n",
            "Image:735 Loss:0.06848635524511337\n",
            "Image:736 Loss:0.07186174392700195\n",
            "Image:737 Loss:0.06694509088993073\n",
            "Image:738 Loss:0.07179094105958939\n",
            "Image:739 Loss:0.06622298806905746\n",
            "Image:740 Loss:0.06937810033559799\n",
            "Image:741 Loss:0.07464832812547684\n",
            "Image:742 Loss:0.06928057223558426\n",
            "Image:743 Loss:0.06785643845796585\n",
            "Image:744 Loss:0.0725117102265358\n",
            "Image:745 Loss:0.07369576394557953\n",
            "Image:746 Loss:0.06822296977043152\n",
            "Image:747 Loss:0.07052218914031982\n",
            "Image:748 Loss:0.07215847074985504\n",
            "Image:749 Loss:0.06608959287405014\n",
            "Image:750 Loss:0.06086471676826477\n",
            "Image:751 Loss:0.07756689190864563\n",
            "Image:752 Loss:0.07084493339061737\n",
            "Image:753 Loss:0.06807579845190048\n",
            "Image:754 Loss:0.06976952403783798\n",
            "Image:755 Loss:0.07193629443645477\n",
            "Image:756 Loss:0.07211866229772568\n",
            "Image:757 Loss:0.06831876188516617\n",
            "Image:758 Loss:0.07444097101688385\n",
            "Image:759 Loss:0.06858852505683899\n",
            "Image:760 Loss:0.06740444898605347\n",
            "Image:761 Loss:0.06889922171831131\n",
            "Image:762 Loss:0.06495581567287445\n",
            "Image:763 Loss:0.0657731220126152\n",
            "Image:764 Loss:0.06801526993513107\n",
            "Image:765 Loss:0.066374272108078\n",
            "Image:766 Loss:0.06496611982584\n",
            "Image:767 Loss:0.06338797509670258\n",
            "Image:768 Loss:0.07062423974275589\n",
            "Image:769 Loss:0.07007201761007309\n",
            "Image:770 Loss:0.0667160376906395\n",
            "Image:771 Loss:0.06950265169143677\n",
            "Image:772 Loss:0.06792100518941879\n",
            "Image:773 Loss:0.06679684668779373\n",
            "Image:774 Loss:0.06588371843099594\n",
            "Image:775 Loss:0.06386315077543259\n",
            "Image:776 Loss:0.0702880248427391\n",
            "Image:777 Loss:0.07081128656864166\n",
            "Image:778 Loss:0.06504261493682861\n",
            "Image:779 Loss:0.07153508812189102\n",
            "Image:780 Loss:0.07133567333221436\n",
            "Image:781 Loss:0.06549391150474548\n",
            "Image:782 Loss:0.06464962661266327\n",
            "Image:783 Loss:0.06567119061946869\n",
            "Image:784 Loss:0.06440068036317825\n",
            "Image:785 Loss:0.06504032760858536\n",
            "Image:786 Loss:0.06547892093658447\n",
            "Image:787 Loss:0.06570553779602051\n",
            "Image:788 Loss:0.0622946172952652\n",
            "Image:789 Loss:0.0652250349521637\n",
            "Image:790 Loss:0.06787226349115372\n",
            "Image:791 Loss:0.06595833599567413\n",
            "Image:792 Loss:0.07631012797355652\n",
            "Image:793 Loss:0.0677393227815628\n",
            "Image:794 Loss:0.06395374983549118\n",
            "Image:795 Loss:0.06810512393712997\n",
            "Image:796 Loss:0.06361531466245651\n",
            "Image:797 Loss:0.06955093890428543\n",
            "Image:798 Loss:0.06656762212514877\n",
            "Image:799 Loss:0.06470680236816406\n",
            "Image:800 Loss:0.0670490562915802\n",
            "Image:801 Loss:0.07413909584283829\n",
            "Image:802 Loss:0.07003212720155716\n",
            "Image:803 Loss:0.07318868488073349\n",
            "Image:804 Loss:0.06891284883022308\n",
            "Image:805 Loss:0.06572632491588593\n",
            "Image:806 Loss:0.06405254453420639\n",
            "Image:807 Loss:0.06891603022813797\n",
            "Image:808 Loss:0.06228946894407272\n",
            "Image:809 Loss:0.06025273725390434\n",
            "Image:810 Loss:0.07192468643188477\n",
            "Image:811 Loss:0.06381546705961227\n",
            "Image:812 Loss:0.07625441253185272\n",
            "Image:813 Loss:0.06896091997623444\n",
            "Image:814 Loss:0.07609312981367111\n",
            "Image:815 Loss:0.066348135471344\n",
            "Image:816 Loss:0.06998662650585175\n",
            "Image:817 Loss:0.0718231052160263\n",
            "Image:818 Loss:0.06351476907730103\n",
            "Image:819 Loss:0.06616228818893433\n",
            "Image:820 Loss:0.06546179205179214\n",
            "Image:821 Loss:0.06774213165044785\n",
            "Image:822 Loss:0.06598054617643356\n",
            "Image:823 Loss:0.06770742684602737\n",
            "Image:824 Loss:0.07473648339509964\n",
            "Image:825 Loss:0.07368797063827515\n",
            "Image:826 Loss:0.07212020456790924\n",
            "Image:827 Loss:0.0716874748468399\n",
            "Image:828 Loss:0.0762690082192421\n",
            "Image:829 Loss:0.0691114068031311\n",
            "Image:830 Loss:0.06841258704662323\n",
            "Image:831 Loss:0.06289910525083542\n",
            "Image:832 Loss:0.06511864066123962\n",
            "Image:833 Loss:0.06798605620861053\n",
            "Image:834 Loss:0.06880310922861099\n",
            "Image:835 Loss:0.07130081951618195\n",
            "Image:836 Loss:0.07241752743721008\n",
            "Image:837 Loss:0.06827498972415924\n",
            "Image:838 Loss:0.06505361944437027\n",
            "Image:839 Loss:0.0654095709323883\n",
            "Image:840 Loss:0.06792186945676804\n",
            "Image:841 Loss:0.06917762756347656\n",
            "Image:842 Loss:0.06664060801267624\n",
            "Image:843 Loss:0.06555791944265366\n",
            "Image:844 Loss:0.06141194328665733\n",
            "Image:845 Loss:0.06999743729829788\n",
            "Image:846 Loss:0.06898879259824753\n",
            "Image:847 Loss:0.07029350847005844\n",
            "Image:848 Loss:0.061667218804359436\n",
            "Image:849 Loss:0.06656109541654587\n",
            "Image:850 Loss:0.0723768025636673\n",
            "Image:851 Loss:0.07045058906078339\n",
            "Image:852 Loss:0.06427814066410065\n",
            "Image:853 Loss:0.06467245519161224\n",
            "Image:854 Loss:0.06848002970218658\n",
            "Image:855 Loss:0.0696907639503479\n",
            "Image:856 Loss:0.07154826819896698\n",
            "Image:857 Loss:0.06588151305913925\n",
            "Image:858 Loss:0.06761348992586136\n",
            "Image:859 Loss:0.06681328266859055\n",
            "Image:860 Loss:0.07108263671398163\n",
            "Image:861 Loss:0.06058048456907272\n",
            "Image:862 Loss:0.07015594840049744\n",
            "Image:863 Loss:0.06995519995689392\n",
            "Image:864 Loss:0.06931775063276291\n",
            "Image:865 Loss:0.06793104112148285\n",
            "Image:866 Loss:0.0714922696352005\n",
            "Image:867 Loss:0.07329437881708145\n",
            "Image:868 Loss:0.07021908462047577\n",
            "Image:869 Loss:0.07110662758350372\n",
            "Image:870 Loss:0.06312042474746704\n",
            "Image:871 Loss:0.06660808622837067\n",
            "Image:872 Loss:0.06944821029901505\n",
            "Image:873 Loss:0.07402195781469345\n",
            "Image:874 Loss:0.07148434221744537\n",
            "Image:875 Loss:0.06743939965963364\n",
            "Image:876 Loss:0.06442373245954514\n",
            "Image:877 Loss:0.057191185653209686\n",
            "Image:878 Loss:0.06758447736501694\n",
            "Image:879 Loss:0.0661202147603035\n",
            "Image:880 Loss:0.06634225696325302\n",
            "Image:881 Loss:0.07589319348335266\n",
            "Image:882 Loss:0.07440491765737534\n",
            "Image:883 Loss:0.07163732498884201\n",
            "Image:884 Loss:0.07292650640010834\n",
            "Image:885 Loss:0.06531000137329102\n",
            "Image:886 Loss:0.06406978517770767\n",
            "Image:887 Loss:0.0666627362370491\n",
            "Image:888 Loss:0.0680585503578186\n",
            "Image:889 Loss:0.07061637192964554\n",
            "Image:890 Loss:0.06483569741249084\n",
            "Image:891 Loss:0.06739218533039093\n",
            "Image:892 Loss:0.06485500931739807\n",
            "Image:893 Loss:0.06840672343969345\n",
            "Image:894 Loss:0.06120261177420616\n",
            "Image:895 Loss:0.06797273457050323\n",
            "Image:896 Loss:0.0768669992685318\n",
            "Image:897 Loss:0.06797496229410172\n",
            "Image:898 Loss:0.06870727241039276\n",
            "Image:899 Loss:0.0659031942486763\n",
            "Image:900 Loss:0.0660635456442833\n",
            "Image:901 Loss:0.06506875157356262\n",
            "Image:902 Loss:0.06583674252033234\n",
            "Image:903 Loss:0.06864019483327866\n",
            "Image:904 Loss:0.06664271652698517\n",
            "Image:905 Loss:0.07102678716182709\n",
            "Image:906 Loss:0.0699903592467308\n",
            "Image:907 Loss:0.06745358556509018\n",
            "Image:908 Loss:0.06889189779758453\n",
            "Image:909 Loss:0.07061510533094406\n",
            "Image:910 Loss:0.06100912764668465\n",
            "Image:911 Loss:0.06337076425552368\n",
            "Image:912 Loss:0.07203402370214462\n",
            "Image:913 Loss:0.07362470030784607\n",
            "Image:914 Loss:0.06528451293706894\n",
            "Image:915 Loss:0.06857036799192429\n",
            "Image:916 Loss:0.06769861280918121\n",
            "Image:917 Loss:0.06433515250682831\n",
            "Image:918 Loss:0.0712423026561737\n",
            "Image:919 Loss:0.06450296193361282\n",
            "Image:920 Loss:0.06227872520685196\n",
            "Image:921 Loss:0.06533794105052948\n",
            "Image:922 Loss:0.07144759595394135\n",
            "Image:923 Loss:0.06823689490556717\n",
            "Image:924 Loss:0.06975816190242767\n",
            "Image:925 Loss:0.06849735230207443\n",
            "Image:926 Loss:0.0755082219839096\n",
            "Image:927 Loss:0.08183412253856659\n",
            "Image:928 Loss:0.06753090023994446\n",
            "Image:929 Loss:0.06633912771940231\n",
            "Image:930 Loss:0.06704659759998322\n",
            "Image:931 Loss:0.06802824139595032\n",
            "Image:932 Loss:0.0652780830860138\n",
            "Image:933 Loss:0.06703149527311325\n",
            "Image:934 Loss:0.07001699507236481\n",
            "Image:935 Loss:0.06762737035751343\n",
            "Image:936 Loss:0.07211437076330185\n",
            "Image:937 Loss:0.07387934625148773\n",
            "Image:938 Loss:0.0756784975528717\n",
            "Image:939 Loss:0.0651131421327591\n",
            "Image:940 Loss:0.06801117956638336\n",
            "Image:941 Loss:0.07451700419187546\n",
            "Image:942 Loss:0.06735026091337204\n",
            "Image:943 Loss:0.07172492891550064\n",
            "Image:944 Loss:0.07012461870908737\n",
            "Image:945 Loss:0.06870579719543457\n",
            "Image:946 Loss:0.06959868222475052\n",
            "Image:947 Loss:0.0674518570303917\n",
            "Image:948 Loss:0.06518365442752838\n",
            "Image:949 Loss:0.0716451033949852\n",
            "Image:950 Loss:0.06619010120630264\n",
            "Image:951 Loss:0.06460089236497879\n",
            "Image:952 Loss:0.06940018385648727\n",
            "Image:953 Loss:0.06899131834506989\n",
            "Image:954 Loss:0.06580037623643875\n",
            "Image:955 Loss:0.06210898235440254\n",
            "Image:956 Loss:0.06114359200000763\n",
            "Image:957 Loss:0.06883839517831802\n",
            "Image:958 Loss:0.071556456387043\n",
            "Image:959 Loss:0.07119536399841309\n",
            "Image:960 Loss:0.06640882045030594\n",
            "Image:961 Loss:0.06840229034423828\n",
            "Image:962 Loss:0.06970705837011337\n",
            "Image:963 Loss:0.06687818467617035\n",
            "Image:964 Loss:0.06877158582210541\n",
            "Image:965 Loss:0.06943020224571228\n",
            "Image:966 Loss:0.0656793937087059\n",
            "Image:967 Loss:0.07335377484560013\n",
            "Image:968 Loss:0.07147610187530518\n",
            "Image:969 Loss:0.06808048486709595\n",
            "Image:970 Loss:0.07304716110229492\n",
            "Image:971 Loss:0.0703795850276947\n",
            "Image:972 Loss:0.06378741562366486\n",
            "Image:973 Loss:0.06721648573875427\n",
            "Image:974 Loss:0.06081024184823036\n",
            "Image:975 Loss:0.07228225469589233\n",
            "Image:976 Loss:0.06491117924451828\n",
            "Image:977 Loss:0.06696682423353195\n",
            "Image:978 Loss:0.06862764060497284\n",
            "Image:979 Loss:0.0724804624915123\n",
            "Image:980 Loss:0.06328008323907852\n",
            "Image:981 Loss:0.07255083322525024\n",
            "Image:982 Loss:0.06921635568141937\n",
            "Image:983 Loss:0.06735388189554214\n",
            "Image:984 Loss:0.0654895082116127\n",
            "Image:985 Loss:0.0674789547920227\n",
            "Image:986 Loss:0.06641896814107895\n",
            "Image:987 Loss:0.06579109281301498\n",
            "Image:988 Loss:0.07573245465755463\n",
            "Image:989 Loss:0.06601832062005997\n",
            "Image:990 Loss:0.07071993499994278\n",
            "Image:991 Loss:0.09089677035808563\n",
            "Image:992 Loss:0.07394245266914368\n",
            "Image:993 Loss:0.07438218593597412\n",
            "Image:994 Loss:0.0703253448009491\n",
            "Image:995 Loss:0.06869786232709885\n",
            "Image:996 Loss:0.06825126707553864\n",
            "Image:997 Loss:0.0693613812327385\n",
            "Image:998 Loss:0.0756983608007431\n",
            "Image:999 Loss:0.06860598921775818\n",
            "Image:1000 Loss:0.07051697373390198\n",
            "Image:1001 Loss:0.06957073509693146\n",
            "Image:1002 Loss:0.06934650987386703\n",
            "Image:1003 Loss:0.07573343813419342\n",
            "Image:1004 Loss:0.07411019504070282\n",
            "Image:1005 Loss:0.07131487131118774\n",
            "Image:1006 Loss:0.07133430987596512\n",
            "Image:1007 Loss:0.0654461532831192\n",
            "Image:1008 Loss:0.06618669629096985\n",
            "Image:1009 Loss:0.06880752742290497\n",
            "Image:1010 Loss:0.06506600975990295\n",
            "Image:1011 Loss:0.07234828174114227\n",
            "Image:1012 Loss:0.07597804814577103\n",
            "Image:1013 Loss:0.06607867777347565\n",
            "Image:1014 Loss:0.06525278836488724\n",
            "Image:1015 Loss:0.07046393305063248\n",
            "Image:1016 Loss:0.06597808003425598\n",
            "Image:1017 Loss:0.06517818570137024\n",
            "Image:1018 Loss:0.06822510808706284\n",
            "Image:1019 Loss:0.07221180200576782\n",
            "Image:1020 Loss:0.06437471508979797\n",
            "Image:1021 Loss:0.06430567055940628\n",
            "Image:1022 Loss:0.07485169917345047\n",
            "Image:1023 Loss:0.06780338287353516\n",
            "Image:1024 Loss:0.06851471215486526\n",
            "Image:1025 Loss:0.06876178830862045\n",
            "Image:1026 Loss:0.061888545751571655\n",
            "Image:1027 Loss:0.0642063096165657\n",
            "Image:1028 Loss:0.06430334597826004\n",
            "Image:1029 Loss:0.06553705036640167\n",
            "Image:1030 Loss:0.06327636539936066\n",
            "Image:1031 Loss:0.06378388404846191\n",
            "Image:1032 Loss:0.06687907874584198\n",
            "Image:1033 Loss:0.0666198879480362\n",
            "Image:1034 Loss:0.06666811555624008\n",
            "Image:1035 Loss:0.0728151798248291\n",
            "Image:1036 Loss:0.06452744454145432\n",
            "Image:1037 Loss:0.06749828159809113\n",
            "Image:1038 Loss:0.0671844482421875\n",
            "Image:1039 Loss:0.07185376435518265\n",
            "Image:1040 Loss:0.06991664320230484\n",
            "Image:1041 Loss:0.06895934790372849\n",
            "Image:1042 Loss:0.06740903109312057\n",
            "Image:1043 Loss:0.07255196571350098\n",
            "Image:1044 Loss:0.07187717407941818\n",
            "Image:1045 Loss:0.0715484768152237\n",
            "Image:1046 Loss:0.07065968960523605\n",
            "Image:1047 Loss:0.0674043744802475\n",
            "Image:1048 Loss:0.06746090203523636\n",
            "Image:1049 Loss:0.06805215775966644\n",
            "Image:1050 Loss:0.06786631047725677\n",
            "Image:1051 Loss:0.06491391360759735\n",
            "Image:1052 Loss:0.0691509023308754\n",
            "Image:1053 Loss:0.06544651091098785\n",
            "Image:1054 Loss:0.06778568774461746\n",
            "Image:1055 Loss:0.06220528483390808\n",
            "Image:1056 Loss:0.07032869011163712\n",
            "Image:1057 Loss:0.07334726303815842\n",
            "Image:1058 Loss:0.07800451666116714\n",
            "Image:1059 Loss:0.07041601836681366\n",
            "Image:1060 Loss:0.06581638008356094\n",
            "Image:1061 Loss:0.07199490070343018\n",
            "Image:1062 Loss:0.06601131707429886\n",
            "Image:1063 Loss:0.07199940085411072\n",
            "Image:1064 Loss:0.07113984227180481\n",
            "Image:1065 Loss:0.06763867288827896\n",
            "Image:1066 Loss:0.0666067823767662\n",
            "Image:1067 Loss:0.07360604405403137\n",
            "Image:1068 Loss:0.06512433290481567\n",
            "Image:1069 Loss:0.07231520116329193\n",
            "Image:1070 Loss:0.06859302520751953\n",
            "Image:1071 Loss:0.07336809486150742\n",
            "Image:1072 Loss:0.06764698028564453\n",
            "Image:1073 Loss:0.06357695907354355\n",
            "Image:1074 Loss:0.07208274304866791\n",
            "Image:1075 Loss:0.06930790841579437\n",
            "Image:1076 Loss:0.07259969413280487\n",
            "Image:1077 Loss:0.06736096739768982\n",
            "Image:1078 Loss:0.0679672583937645\n",
            "Image:1079 Loss:0.07284631580114365\n",
            "Image:1080 Loss:0.070670485496521\n",
            "Image:1081 Loss:0.06499568372964859\n",
            "Image:1082 Loss:0.06945968419313431\n",
            "Image:1083 Loss:0.07144128531217575\n",
            "Image:1084 Loss:0.07444743812084198\n",
            "Image:1085 Loss:0.06480009853839874\n",
            "Image:1086 Loss:0.07220442593097687\n",
            "Image:1087 Loss:0.06521472334861755\n",
            "Image:1088 Loss:0.06810161471366882\n",
            "Image:1089 Loss:0.06979785859584808\n",
            "Image:1090 Loss:0.07120031118392944\n",
            "Image:1091 Loss:0.0626077726483345\n",
            "Image:1092 Loss:0.06947165727615356\n",
            "Image:1093 Loss:0.06847403198480606\n",
            "Image:1094 Loss:0.06628785282373428\n",
            "Image:1095 Loss:0.06783346831798553\n",
            "Image:1096 Loss:0.06316708028316498\n",
            "Image:1097 Loss:0.0719994381070137\n",
            "Image:1098 Loss:0.06595147401094437\n",
            "Image:1099 Loss:0.0680055022239685\n",
            "Image:1100 Loss:0.07351719588041306\n",
            "Image:1101 Loss:0.06865357607603073\n",
            "Image:1102 Loss:0.06715529412031174\n",
            "Image:1103 Loss:0.07074947655200958\n",
            "Image:1104 Loss:0.0656861811876297\n",
            "Image:1105 Loss:0.07602004706859589\n",
            "Image:1106 Loss:0.07017126679420471\n",
            "Image:1107 Loss:0.07078805565834045\n",
            "Image:1108 Loss:0.07027753442525864\n",
            "Image:1109 Loss:0.07189828902482986\n",
            "Image:1110 Loss:0.07153325527906418\n",
            "Image:1111 Loss:0.06723156571388245\n",
            "Image:1112 Loss:0.06170574948191643\n",
            "Image:1113 Loss:0.06947074830532074\n",
            "Image:1114 Loss:0.07684311270713806\n",
            "Image:1115 Loss:0.07208210229873657\n",
            "Image:1116 Loss:0.06663085520267487\n",
            "Image:1117 Loss:0.07484350353479385\n",
            "Image:1118 Loss:0.06712187081575394\n",
            "Image:1119 Loss:0.07111650705337524\n",
            "Image:1120 Loss:0.07002422958612442\n",
            "Image:1121 Loss:0.07154256105422974\n",
            "Image:1122 Loss:0.06801049411296844\n",
            "Image:1123 Loss:0.07247988879680634\n",
            "Image:1124 Loss:0.064185231924057\n",
            "Image:1125 Loss:0.06894990056753159\n",
            "Image:1126 Loss:0.06901451200246811\n",
            "Image:1127 Loss:0.06930983811616898\n",
            "Image:1128 Loss:0.07118752598762512\n",
            "Image:1129 Loss:0.05893540754914284\n",
            "Image:1130 Loss:0.06387338042259216\n",
            "Image:1131 Loss:0.06835909932851791\n",
            "Image:1132 Loss:0.06874988973140717\n",
            "Image:1133 Loss:0.07285841554403305\n",
            "Image:1134 Loss:0.07123924791812897\n",
            "Image:1135 Loss:0.06397892534732819\n",
            "Image:1136 Loss:0.0616798959672451\n",
            "Image:1137 Loss:0.07111383229494095\n",
            "Image:1138 Loss:0.07536313682794571\n",
            "Image:1139 Loss:0.06552842259407043\n",
            "Image:1140 Loss:0.07015123963356018\n",
            "Image:1141 Loss:0.06843731552362442\n",
            "Image:1142 Loss:0.06862743198871613\n",
            "Image:1143 Loss:0.0709858313202858\n",
            "Image:1144 Loss:0.07205991446971893\n",
            "Image:1145 Loss:0.06764181703329086\n",
            "Image:1146 Loss:0.06860440224409103\n",
            "Image:1147 Loss:0.06802241504192352\n",
            "Image:1148 Loss:0.06449782848358154\n",
            "Image:1149 Loss:0.06708825379610062\n",
            "Image:1150 Loss:0.06960097700357437\n",
            "Image:1151 Loss:0.06477395445108414\n",
            "Image:1152 Loss:0.06417197734117508\n",
            "Image:1153 Loss:0.06887570023536682\n",
            "Image:1154 Loss:0.0678417831659317\n",
            "Image:1155 Loss:0.0660984143614769\n",
            "Image:1156 Loss:0.06568966805934906\n",
            "Image:1157 Loss:0.06404554098844528\n",
            "Image:1158 Loss:0.066149041056633\n",
            "Image:1159 Loss:0.07507104426622391\n",
            "Image:1160 Loss:0.06863926351070404\n",
            "Image:1161 Loss:0.06927559524774551\n",
            "Image:1162 Loss:0.0604017972946167\n",
            "Image:1163 Loss:0.06561078876256943\n",
            "Image:1164 Loss:0.06252801418304443\n",
            "Image:1165 Loss:0.06804501265287399\n",
            "Image:1166 Loss:0.06627480685710907\n",
            "Image:1167 Loss:0.06643148511648178\n",
            "Image:1168 Loss:0.0660146102309227\n",
            "Image:1169 Loss:0.06814979761838913\n",
            "Image:1170 Loss:0.06526946276426315\n",
            "Image:1171 Loss:0.06964575499296188\n",
            "Image:1172 Loss:0.06862376630306244\n",
            "Image:1173 Loss:0.06340111047029495\n",
            "Image:1174 Loss:0.06821321696043015\n",
            "Image:1175 Loss:0.07033087313175201\n",
            "Image:1176 Loss:0.06541406363248825\n",
            "Image:1177 Loss:0.06784774363040924\n",
            "Image:1178 Loss:0.06769131869077682\n",
            "Image:1179 Loss:0.07178497314453125\n",
            "Image:1180 Loss:0.07321043312549591\n",
            "Image:1181 Loss:0.07067167013883591\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1a4c3092cee3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch:{epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;31m# Reshaping the image to (-1, 784)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# handle PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I;16\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0;32mclass\u001b[0m \u001b[0mArrayData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m             \u001b[0m__array_interface__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Plot Style\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        " \n",
        "# Plotting the last 100 values\n",
        "plt.plot(losses[-100:].cuda())"
      ],
      "metadata": {
        "id": "SY7qXmD4WHM9",
        "outputId": "b591be40-d960-4069-8335-01dba3455d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e25fabbc5ef8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Plotting the last 100 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'cuda'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(image):\n",
        "   \n",
        "  # Reshape the array for plotting\n",
        "  item = item.reshape(-1, 28, 28)\n",
        "  plt.imshow(item[0])\n",
        " \n",
        "for i, item in enumerate(reconstructed):\n",
        "  item = item.reshape(-1, 28, 28)\n",
        "  plt.imshow(item[0])"
      ],
      "metadata": {
        "id": "S8WuDfA8SDiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNkZe7zwTmx0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}